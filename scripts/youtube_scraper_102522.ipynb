{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8b9472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jetcalz07/opt/miniconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import spacy\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_helpers import get_channel_playlists, get_video_ids, get_video_details\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "path = '/Users/jetcalz07/Desktop/MIDS/W266_NLP/nlp_podcast_segmentation/'\n",
    "load_scripts = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60517180",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyDTr6GYriQpML3rwVB_M4aVhixseOxVO4U'\n",
    "channel_ids = ['UCSHZKyawb77ixDdsGog4iWA', # Lex Fridman\n",
    "               'UCESLZhusAkFfsNsApnjF_Cg', # All-In\n",
    "              ]\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc8d5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:04<00:16,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match\n",
      "final_sentence_inds length: 384\n",
      "final_sent_word_lists length: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:07<00:11,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_sentence_inds length: 381\n",
      "final_sent_word_lists length: 380\n",
      "Mismatch of sentence inds issue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:11<00:07,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match\n",
      "final_sentence_inds length: 340\n",
      "final_sent_word_lists length: 341\n",
      "Mismatch of sentence inds issue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:14<00:03,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match\n",
      "final_sentence_inds length: 351\n",
      "final_sent_word_lists length: 351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:18<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match\n",
      "final_sentence_inds length: 332\n",
      "final_sent_word_lists length: 332\n",
      "Total Channels: 2, Total Videos: 836\n",
      "Total videos with timestamps & scripts: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Transcript-Source</th>\n",
       "      <th>Word_List</th>\n",
       "      <th>Word_Time_List</th>\n",
       "      <th>All_Words</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Sentence_Word_Lists</th>\n",
       "      <th>Transition_Labels</th>\n",
       "      <th>Segment_Times</th>\n",
       "      <th>Segment_Times_Secs</th>\n",
       "      <th>Segments_All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All-In Podcast</td>\n",
       "      <td>E101: Ye acquires Parler, Snap drops 30%, macr...</td>\n",
       "      <td>(0:00) Bestie intros: Brad Gerstner joins in p...</td>\n",
       "      <td>[{'text': 'welcome everybody to episode 101 de...</td>\n",
       "      <td>auto</td>\n",
       "      <td>[welcome, everybody, to, episode, 101, deutsax...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.419, 3.419, 3...</td>\n",
       "      <td>welcome everybody to episode 101 deutsax is o...</td>\n",
       "      <td>[welcome everybody to episode 101 deutsax, is ...</td>\n",
       "      <td>[[welcome, everybody, to, episode, 101, deutsa...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0:00, 2:02, 4:17, 2:53, 1:05:38, 1:21:38]</td>\n",
       "      <td>[0.0, 122.0, 257.0, 173.0, 3938.0, 4898.0]</td>\n",
       "      <td>[(0:00) Bestie intros: Brad Gerstner joins in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All-In Podcast</td>\n",
       "      <td>E100: Reflecting on the first 100 shows, fan q...</td>\n",
       "      <td>(0:00) Bestie intros are back!\\n(6:35) Reflect...</td>\n",
       "      <td>[{'text': 'Jake out do you have intros today n...</td>\n",
       "      <td>auto</td>\n",
       "      <td>[Jake, out, do, you, have, intros, today, no, ...</td>\n",
       "      <td>[0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.2...</td>\n",
       "      <td>Jake out do you have intros today no I dont h...</td>\n",
       "      <td>[Jake out do you have intros today no I, don't...</td>\n",
       "      <td>[[Jake, out, do, you, have, intros, today, no,...</td>\n",
       "      <td>None</td>\n",
       "      <td>[0:00, 6:35, 9:12, 3:43, 1:14:57]</td>\n",
       "      <td>[0.0, 395.0, 552.0, 223.0, 4497.0]</td>\n",
       "      <td>[(0:00) Bestie intros are back!, (6:35) Reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All-In Podcast</td>\n",
       "      <td>E99: Cheating scandals, Twitter updates, rapid...</td>\n",
       "      <td>0:00 Bestie intros!\\n1:34 Breaking down major ...</td>\n",
       "      <td>[{'text': 'we're seven minutes in and we've', ...</td>\n",
       "      <td>auto</td>\n",
       "      <td>[were, seven, minutes, in, and, weve, produced...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.92, 1.92, 1.9...</td>\n",
       "      <td>were seven minutes in and weve produced absol...</td>\n",
       "      <td>[we're seven minutes in and we've, produced ab...</td>\n",
       "      <td>[[], [were, seven, minutes, in, and, weve, pro...</td>\n",
       "      <td>None</td>\n",
       "      <td>[0:00, 1:34, 6:13, 0:05, 9:45, 9:24, 0:00, 1:3...</td>\n",
       "      <td>[0.0, 94.0, 373.0, 5.0, 585.0, 564.0, 0.0, 94....</td>\n",
       "      <td>[0:00 Bestie intros!, 1:34 Breaking down major...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All-In Podcast</td>\n",
       "      <td>E98: Big tech starts making cuts, Fed incompet...</td>\n",
       "      <td>0:00 Bestie intros!\\n1:36 Big tech starts maki...</td>\n",
       "      <td>[{'text': 'hey everybody Welcome to episode 98...</td>\n",
       "      <td>auto</td>\n",
       "      <td>[hey, everybody, Welcome, to, episode, 98, of,...</td>\n",
       "      <td>[0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 2.7...</td>\n",
       "      <td>hey everybody Welcome to episode 98 of the al...</td>\n",
       "      <td>[hey everybody Welcome to episode 98 of, the a...</td>\n",
       "      <td>[[hey, everybody, Welcome, to, episode, 98, of...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0:00, 1:36, 1:09, 4:06, 1:00:30]</td>\n",
       "      <td>[0.0, 96.0, 69.0, 246.0, 3630.0]</td>\n",
       "      <td>[0:00 Bestie intros!, 1:36 Big tech starts mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All-In Podcast</td>\n",
       "      <td>E97: SPAC updates, public/private market overv...</td>\n",
       "      <td>0:00 Bestie intros!\\n1:03 SPAC updates, state ...</td>\n",
       "      <td>[{'text': 'I'm in a very Daniel Plainview mood...</td>\n",
       "      <td>auto</td>\n",
       "      <td>[Im, in, a, very, Daniel, Plainview, mood, thi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.399...</td>\n",
       "      <td>Im in a very Daniel Plainview mood this week ...</td>\n",
       "      <td>[I'm in a very Daniel Plainview mood this, wee...</td>\n",
       "      <td>[[Im, in, a, very, Daniel, Plainview, mood, th...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0:00, 1:03, 0:26, 1:14, 0:45, 1:01:20]</td>\n",
       "      <td>[0.0, 63.0, 26.0, 74.0, 45.0, 3680.0]</td>\n",
       "      <td>[0:00 Bestie intros!, 1:03 SPAC updates, state...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Channel                                              Title  \\\n",
       "0  All-In Podcast  E101: Ye acquires Parler, Snap drops 30%, macr...   \n",
       "1  All-In Podcast  E100: Reflecting on the first 100 shows, fan q...   \n",
       "2  All-In Podcast  E99: Cheating scandals, Twitter updates, rapid...   \n",
       "3  All-In Podcast  E98: Big tech starts making cuts, Fed incompet...   \n",
       "4  All-In Podcast  E97: SPAC updates, public/private market overv...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  (0:00) Bestie intros: Brad Gerstner joins in p...   \n",
       "1  (0:00) Bestie intros are back!\\n(6:35) Reflect...   \n",
       "2  0:00 Bestie intros!\\n1:34 Breaking down major ...   \n",
       "3  0:00 Bestie intros!\\n1:36 Big tech starts maki...   \n",
       "4  0:00 Bestie intros!\\n1:03 SPAC updates, state ...   \n",
       "\n",
       "                                          Transcript Transcript-Source  \\\n",
       "0  [{'text': 'welcome everybody to episode 101 de...              auto   \n",
       "1  [{'text': 'Jake out do you have intros today n...              auto   \n",
       "2  [{'text': 'we're seven minutes in and we've', ...              auto   \n",
       "3  [{'text': 'hey everybody Welcome to episode 98...              auto   \n",
       "4  [{'text': 'I'm in a very Daniel Plainview mood...              auto   \n",
       "\n",
       "                                           Word_List  \\\n",
       "0  [welcome, everybody, to, episode, 101, deutsax...   \n",
       "1  [Jake, out, do, you, have, intros, today, no, ...   \n",
       "2  [were, seven, minutes, in, and, weve, produced...   \n",
       "3  [hey, everybody, Welcome, to, episode, 98, of,...   \n",
       "4  [Im, in, a, very, Daniel, Plainview, mood, thi...   \n",
       "\n",
       "                                      Word_Time_List  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.419, 3.419, 3...   \n",
       "1  [0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.2...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.92, 1.92, 1.9...   \n",
       "3  [0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 2.7...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.399...   \n",
       "\n",
       "                                           All_Words  \\\n",
       "0   welcome everybody to episode 101 deutsax is o...   \n",
       "1   Jake out do you have intros today no I dont h...   \n",
       "2   were seven minutes in and weve produced absol...   \n",
       "3   hey everybody Welcome to episode 98 of the al...   \n",
       "4   Im in a very Daniel Plainview mood this week ...   \n",
       "\n",
       "                                           Sentences  \\\n",
       "0  [welcome everybody to episode 101 deutsax, is ...   \n",
       "1  [Jake out do you have intros today no I, don't...   \n",
       "2  [we're seven minutes in and we've, produced ab...   \n",
       "3  [hey everybody Welcome to episode 98 of, the a...   \n",
       "4  [I'm in a very Daniel Plainview mood this, wee...   \n",
       "\n",
       "                                 Sentence_Word_Lists  \\\n",
       "0  [[welcome, everybody, to, episode, 101, deutsa...   \n",
       "1  [[Jake, out, do, you, have, intros, today, no,...   \n",
       "2  [[], [were, seven, minutes, in, and, weve, pro...   \n",
       "3  [[hey, everybody, Welcome, to, episode, 98, of...   \n",
       "4  [[Im, in, a, very, Daniel, Plainview, mood, th...   \n",
       "\n",
       "                                   Transition_Labels  \\\n",
       "0  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "4  [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                       Segment_Times  \\\n",
       "0         [0:00, 2:02, 4:17, 2:53, 1:05:38, 1:21:38]   \n",
       "1                  [0:00, 6:35, 9:12, 3:43, 1:14:57]   \n",
       "2  [0:00, 1:34, 6:13, 0:05, 9:45, 9:24, 0:00, 1:3...   \n",
       "3                  [0:00, 1:36, 1:09, 4:06, 1:00:30]   \n",
       "4            [0:00, 1:03, 0:26, 1:14, 0:45, 1:01:20]   \n",
       "\n",
       "                                  Segment_Times_Secs  \\\n",
       "0         [0.0, 122.0, 257.0, 173.0, 3938.0, 4898.0]   \n",
       "1                 [0.0, 395.0, 552.0, 223.0, 4497.0]   \n",
       "2  [0.0, 94.0, 373.0, 5.0, 585.0, 564.0, 0.0, 94....   \n",
       "3                   [0.0, 96.0, 69.0, 246.0, 3630.0]   \n",
       "4              [0.0, 63.0, 26.0, 74.0, 45.0, 3680.0]   \n",
       "\n",
       "                                        Segments_All  \n",
       "0  [(0:00) Bestie intros: Brad Gerstner joins in ...  \n",
       "1  [(0:00) Bestie intros are back!, (6:35) Reflec...  \n",
       "2  [0:00 Bestie intros!, 1:34 Breaking down major...  \n",
       "3  [0:00 Bestie intros!, 1:36 Big tech starts mak...  \n",
       "4  [0:00 Bestie intros!, 1:03 SPAC updates, state...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Execute functions to collect data from Youtube channels\n",
    "\n",
    "if not load_scripts:\n",
    "    # Get playlist ids for channel\n",
    "    channel_df = pd.DataFrame(get_channel_playlists(youtube, channel_ids))\n",
    "\n",
    "    # Loop through channels, get video ids\n",
    "    v_ids = np.array([])\n",
    "    for idx, row in channel_df.iterrows():\n",
    "        playlist_id = row['playlist_id']\n",
    "        channel_vids = get_video_ids(youtube, playlist_id)\n",
    "        v_ids = np.append(v_ids, channel_vids)\n",
    "\n",
    "    # Loop through videos, get timestamp, script information\n",
    "    vid_details = pd.DataFrame(get_video_details(youtube, v_ids[:5], splitter='spacy', nlp = nlp)) #<-------------------------- remove the 5 when done writing\n",
    "    print(f\"Total Channels: {len(channel_df)}, Total Videos: {len(v_ids)}\")\n",
    "    print(f\"Total videos with timestamps & scripts: {len(vid_details)}\")\n",
    "    #vid_details.to_csv(path+'data/yt_scripts_segments_v1.csv', index=False)\n",
    "\n",
    "else:\n",
    "    vid_details = pd.read_csv(path+'data/yt_scripts_segments_v1.csv')\n",
    "    vid_details['Transcript'] = vid_details['Transcript'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "vid_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42071e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "True\n",
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m idx, row \u001b[39min\u001b[39;00m vid_details\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(idx)\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mSentence_Word_Lists\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39;49m(row[\u001b[39m'\u001b[39;49m\u001b[39mTransition_Labels\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for idx, row in vid_details.iterrows():\n",
    "    print(idx)\n",
    "    print(len(row['Sentence_Word_Lists']) == len(row['Transition_Labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5de8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff42e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c786e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c574d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0f6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "143ab040",
   "metadata": {},
   "source": [
    "## Split scripts into sentences w/ SentenceBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549616ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack script into string of words, and word-time mapping\n",
    "def unpack_words(script, replace_list = [\"'\", \"_\", \"[\", \"]\"]):\n",
    "    utter_words = []\n",
    "    utter_times = []\n",
    "    all_text = ''\n",
    "\n",
    "    for line in script:\n",
    "        for word in line['text'].split(\" \"):\n",
    "            if (word != \"\"):\n",
    "                for str in replace_list:\n",
    "                    word = word.replace(str,\"\")\n",
    "                utter_words.append(word)\n",
    "                utter_times.append(line['start'])\n",
    "                all_text = all_text + ' ' + word\n",
    "            \n",
    "    return utter_words, utter_times, all_text\n",
    "\n",
    "#sentence splitting with Spacy\n",
    "def spacy_split(nlp, all_text):\n",
    "    about_doc = nlp(all_text)\n",
    "    sentences = list(about_doc.sents)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# after sentence split, map each word to a sentence ind\n",
    "def word_sentence_map(sentences):\n",
    "    z = 0 # sentence index\n",
    "    sent_inds = []\n",
    "    sent_word_lists = [] # store the words for each sentence\n",
    "    sent_words = [] # all words after splitting\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_list = []\n",
    "        for word in sentence.text.split(\" \"):\n",
    "            if (str(word) == \"\") | (str(word) == \" \") | (str(word) == \"\\n\") | (str(word) == \"_\") | (str(word) == \"[\") | (str(word) == \"]\"):\n",
    "                pass\n",
    "            else:\n",
    "                sent_inds.append(z)\n",
    "                sent_words.append(word)\n",
    "                word_list.append(word)\n",
    "        z += 1\n",
    "        sent_word_lists.append(word_list)\n",
    "\n",
    "    return sent_words, sent_inds, sent_word_lists\n",
    "\n",
    "## Account for differences in word lists pre/post splitter\n",
    "# Basically check the text and if a word does't equal the neighbor, check if the next\n",
    "def map_sentence_time(utter_words, sent_words, utter_times, sent_inds, word_retention_thresh = 0.97):\n",
    "    # check if the utter words and sent words map\n",
    "    if np.all(utter_words == sent_words):\n",
    "        print(f\"Match\")\n",
    "        final_words = utter_words\n",
    "        final_sentence_inds = sent_inds\n",
    "        final_times = utter_times\n",
    "\n",
    "    else:\n",
    "        rows = np.min([len(utter_words), len(sent_words)])\n",
    "        w_t = utter_words\n",
    "        w_s = sent_words\n",
    "        final_words = []\n",
    "        final_sentence_inds = []\n",
    "        final_times = []\n",
    "        final_sent_word_lists = []\n",
    "\n",
    "        w_t_i = 0 # utter word ind\n",
    "        w_s_i = 0 # sentence word ind\n",
    "        check_n = 5 # if misaligned, check words in +=n direction to re-align\n",
    "        sent_ind = -1 # trigger new sentence list at beginning\n",
    "        \n",
    "        for i in range(rows): # loop through all words\n",
    "            # check if new sentence start\n",
    "            if (sent_inds[w_s_i] != sent_ind):\n",
    "                if sent_inds[w_s_i] != 0:\n",
    "                    final_sent_word_lists.append(sent_word_list)\n",
    "                sent_word_list = []\n",
    "                sent_ind += 1\n",
    "\n",
    "            # check if the two lists are equal at indices\n",
    "            if w_t[w_t_i] == w_s[w_s_i]:\n",
    "                final_words.append(w_t[w_t_i])\n",
    "                final_sentence_inds.append(sent_inds[w_s_i])\n",
    "                final_times.append(utter_times[w_t_i])\n",
    "                sent_word_list.append(w_t[w_t_i])\n",
    "                w_t_i += 1\n",
    "                w_s_i += 1\n",
    "\n",
    "            else:\n",
    "                # check if utter word list gets ahead, if so, look at next n sentence map words to adjust inds and re-align\n",
    "                next_w_s = w_s[w_s_i+1] # something is wrong at current index, so lets skip it and go to next word\n",
    "                check_w_t_next = w_t[w_t_i+1:w_t_i+check_n]\n",
    "                if next_w_s in check_w_t_next: # if left list gets ahead, find out by how much, adjust indices, return to top of loop and should get match\n",
    "                    add_ind = np.where(check_w_t_next == next_w_s)[0][0] # take first ind where match is found\n",
    "                    w_t_i += 1 + add_ind # skip ahead of sentence word list\n",
    "                    w_s_i += 1\n",
    "\n",
    "                # check if sentence map gets ahead\n",
    "                next_w_t = w_t[w_t_i+1]\n",
    "                check_w_s_next = np.array(w_s[w_s_i+1:w_s_i+check_n])\n",
    "                if next_w_t in check_w_s_next: # if right list gets ahead, find out by how much, adjust indices, return to top of loop and should get match\n",
    "                    add_ind = np.where(check_w_s_next == next_w_t)[0][0]\n",
    "                    w_s_i += 1 + add_ind # skip ahead of sentence word list\n",
    "                    w_t_i += 1\n",
    "\n",
    "\n",
    "    # Track how many words retained from initial script, ensure > 98%\n",
    "    retention = len(final_words)/len(utter_words)\n",
    "    if retention < word_retention_thresh:\n",
    "        print(\"Retention problem\")\n",
    "        s_t = [None]\n",
    "    else:\n",
    "        s_t_df = pd.DataFrame({'word': final_words, 'sentence_ind': final_sentence_inds, 'time': final_times})\n",
    "        s_t = pd.DataFrame((s_t_df.groupby(['sentence_ind'])['time'].first())).reset_index()\n",
    "        s_t = np.array(s_t['time'])\n",
    "        \n",
    "    return s_t, final_sent_word_lists\n",
    "\n",
    "# after splitting and mapping words-times-sentences, return model input and output labels\n",
    "def get_transition_labels(script, segment_times, splitter='yt_simple'):\n",
    "    if splitter == 'yt_simple':\n",
    "        # parse transcript, get sentences and the corresponding times\n",
    "        sentence_times = [round(line['start']) for line in script]\n",
    "\n",
    "    if splitter == 'spacy':\n",
    "        utter_words, utter_times, all_text = unpack_words(script)\n",
    "        sentences = spacy_split(nlp, all_text)\n",
    "        sent_words, sent_inds, sent_word_lists = word_sentence_map(sentences)\n",
    "        f_s_t, f_s_w_l = map_sentence_time(utter_words, sent_words, utter_times, sent_inds)\n",
    "\n",
    "    # for each timestamp, get the index of the closest utterance, assign transition label\n",
    "    if (None in f_s_t) | (len(f_s_t) != len(f_s_w_l)):\n",
    "        transitions = None\n",
    "    else:\n",
    "        transitions = np.zeros(len(f_s_t))\n",
    "        for t in segment_times:\n",
    "            closest_idx = (np.abs(f_s_t - t)).argmin()\n",
    "            transitions[closest_idx] = 1\n",
    "    \n",
    "\n",
    "    return f_s_w_l, transitions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f685d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = len(vid_details)-1\n",
    "script = vid_details.loc[i, 'Transcript']\n",
    "inputs, outputs = get_transition_labels(script, vid_details.loc[i, 'Segment_Times_Secs'], splitter='spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfaf8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "766380c61e6eecc7f361b76fca150175cbf549793dc3a73d27fd98d75ec4b455"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
