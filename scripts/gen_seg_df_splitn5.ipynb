{"cells":[{"cell_type":"code","source":["# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhHlstvNvrRs","executionInfo":{"status":"ok","timestamp":1669497029514,"user_tz":420,"elapsed":19141,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"}},"outputId":"534369be-3504-4288-9777-2f5cb4c03257"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unYsmq8ktHS2"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import os\n","from tqdm import tqdm\n","import ast\n","import json\n","import random\n","from tensorflow.keras.utils import pad_sequences\n","\n","path = '/content/drive/MyDrive/nlp_podcast_segmentation/'\n","yt_path = path + 'data/YouTube/'\n","splits = 'yt_scripts_segments_split_n5_111422'\n","pooling = 'all-MiniLM-L6-v2-meanpooling' # all-MiniLM-L6-v2-meanpooling or #stsb-mpnet-base-v2-meanpooling\n","embed_path = yt_path + f'/embeddings/{splits}/{pooling}/'"]},{"cell_type":"markdown","source":["## Split Episodes"],"metadata":{"id":"yWmDtl82WQg1"}},{"cell_type":"code","source":["# Load episode df, then merge embedding file paths\n","df = pd.read_pickle(yt_path+'yt_scripts_segments_split_n5_111422_slimcols.csv')\n","emb_df = pd.DataFrame({'Video_Id': [x[:-3] for x in os.listdir(embed_path)],\n","                      'emb_file': os.listdir(embed_path),})\n","vids = pd.merge(df, emb_df, on='Video_Id', how='inner')\n","\n","# Get rid of episodes with only a few segments\n","vids['num_segments'] = vids['Transition_Labels'].apply(lambda x: sum(x))\n","vids = vids.loc[vids['num_segments'] > 4, ].copy()\n","\n","## Split into train and test sets\n","mask = np.random.rand(len(vids)) < 0.5 # set split to 50%\n","np.save(f\"{embed_path}split_mask.npy\", mask)\n","print(\"Generating and saving split mask\")\n","\n","train = vids[mask]\n","test = vids[~mask]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMEsKxE7WBiz","executionInfo":{"status":"ok","timestamp":1669497086390,"user_tz":420,"elapsed":39237,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"}},"outputId":"19f09d85-e23d-4ddc-933b-70a4bdf2c14f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating and saving split mask\n"]}]},{"cell_type":"markdown","source":["## Create Segment-Level DF"],"metadata":{"id":"IBQ-fKm17A3H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2483XZn3tHS7","executionInfo":{"status":"ok","timestamp":1669498473696,"user_tz":420,"elapsed":1371502,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e0f7e9b-5232-4596-9590-cf077dbcba7f"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 453/1596 [04:26<11:58,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Appending chunk to df on disk\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▌    | 887/1596 [09:40<06:39,  1.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Appending chunk to df on disk\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████▏  | 1154/1596 [13:33<06:14,  1.18it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Appending chunk to df on disk\n"]},{"output_type":"stream","name":"stderr","text":[" 81%|████████  | 1294/1596 [16:16<04:15,  1.18it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Appending chunk to df on disk\n"]},{"output_type":"stream","name":"stderr","text":[" 91%|█████████ | 1453/1596 [19:02<01:20,  1.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Appending chunk to df on disk\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1596/1596 [21:13<00:00,  1.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Num segments: 27855\n"]}],"source":["# Init df\n","seg_df = pd.DataFrame(columns=['vid_id', 'seg', 'seg_embs', 'seg_labels'])\n","seg_df.to_csv(embed_path+\"seg_df.csv\", index=False)\n","\n","# Loop through train episodes\n","num_segs = 0\n","for idx, row in tqdm(train.iterrows(), total=train.shape[0]):\n","  embs = torch.load(embed_path + row['emb_file']).numpy().tolist() # <----- need smaller embeddings\n","  labels = row['Transition_Labels'].tolist()\n","\n","  # Get transition inds\n","  inds = np.where(np.array(labels)==1)[0]\n","  inds = np.append(inds, len(labels))\n","\n","  # Loop through inds\n","  for i in range(len(inds)-1):\n","    ind_s = inds[i]\n","    ind_e = inds[i+1]\n","    seg_l = labels[ind_s:ind_e]\n","    seg_e = embs[ind_s:ind_e]\n","\n","    # create df row\n","    seg_row = [row['Video_Id'], i, seg_e, seg_l]\n","    seg_df.loc[len(seg_df)] = seg_row\n","    num_segs += 1\n","  \n","    # append to df to save RAM, restart df in RAM\n","    if (num_segs%5000==0):\n","      print(\"\\nAppending chunk to df on disk\")\n","      seg_df.to_csv(embed_path+\"seg_df.csv\", mode='a', header=False, index=False)\n","      seg_df = pd.DataFrame(columns=['vid_id', 'seg', 'seg_embs', 'seg_labels'])\n","\n","# Save last set of segments\n","seg_df.to_csv(embed_path+\"seg_df.csv\", mode='a', header=False, index=False)\n","print(f\"Num segments: {num_segs}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"u-6-gpogxo_9"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('nlp')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"766380c61e6eecc7f361b76fca150175cbf549793dc3a73d27fd98d75ec4b455"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}